{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "images = np.load(\"D:/work/JHUschoolStuff/machinelearning/project1/cs475_project_data/images.npy\")\n",
    "labels = np.load(\"D:/work/JHUschoolStuff/machinelearning/project1/cs475_project_data/labels.npy\")\n",
    "test = np.load(\"D:/work/JHUschoolStuff/machinelearning/project1/cs475_project_data/test_images.npy\")\n",
    "height = images.shape[1]\n",
    "width = images.shape[2]\n",
    "size = height * width\n",
    "images = (images - images.mean()) / images.std()\n",
    "data = images.reshape(images.shape[0],size)\n",
    "data = torch.from_numpy(data).float().cuda()\n",
    "labels = torch.from_numpy(labels).float().cuda()\n",
    "test_data = test.reshape(test.shape[0], size)\n",
    "test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "batch_size = 1\n",
    "NUM_OPT_STEPS = 5000\n",
    "train_seqs, train_labels = data[0:45000,:], labels[0:45000]\n",
    "val_seqs, val_labels = data[45000:,:], labels[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(height * width, 100)\n",
    "        self.layer_2 = torch.nn.Linear(100, 5)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        y = F.relu(x)\n",
    "        z = self.layer_2(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreeLayerNN(torch.nn.Module):\n",
    "    def __init__(self, layer_1, layer_2):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(height * width, layer_1)\n",
    "        self.layer_2 = torch.nn.Linear(layer_1, layer_2)\n",
    "        self.layer_3 = torch.nn.Linear(layer_2, 5)\n",
    "        #self.drop = torch.nn.Dropout(p = 0.3)\n",
    "    def forward(self, x):\n",
    "        #x = self.drop(x)\n",
    "        x = self.layer_1(x)\n",
    "        y = F.relu(x)\n",
    "        z = self.layer_2(y)\n",
    "        #z = self.drop(z)\n",
    "        a = F.relu(z)\n",
    "        b = self.layer_3(a)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, batch_size):\n",
    "    # model.train() puts our model in train mode, which can require different\n",
    "    # behavior than eval mode (for example in the case of dropout).\n",
    "    model.train()\n",
    "    \n",
    "    # i is is a 1-D array with shape [batch_size]\n",
    "    i = np.random.choice(train_seqs.shape[0], size=batch_size, replace=False)\n",
    "    i = torch.from_numpy(i).long().cuda()\n",
    "    x = autograd.Variable(train_seqs[i, :])\n",
    "    y = autograd.Variable(train_labels[i]).long()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat_ = model(x)\n",
    "    loss = F.cross_entropy(y_hat_, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    return (y == y_hat).astype(np.float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def approx_train_accuracy(model):\n",
    "    i = np.random.choice(train_seqs.shape[0], size=1000, replace=False)\n",
    "    i = torch.from_numpy(i).long().cuda()\n",
    "    x = autograd.Variable(train_seqs[i, :])\n",
    "    y = autograd.Variable(train_labels[i]).long()\n",
    "    y_hat_ = model(x)\n",
    "    pred = []\n",
    "    for j in range(y_hat_.size()[0]):\n",
    "        logits = y_hat_[j,:].cpu().data.numpy()\n",
    "        pred.append(np.argmax(logits))\n",
    "    return accuracy(pred, y.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_accuracy(model):\n",
    "    x = autograd.Variable(val_seqs)\n",
    "    y = autograd.Variable(val_labels)\n",
    "    y_hat_ = model(x)\n",
    "    pred = []\n",
    "    for j in range(y_hat_.size()[0]):\n",
    "        logits = y_hat_[j,:].cpu().data.numpy()\n",
    "        pred.append(np.argmax(logits))\n",
    "    return accuracy(pred, y.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(tr, v):\n",
    "    ind = list(range(len(tr)))\n",
    "    plt.plot(ind,tr,'-ro')\n",
    "    plt.title('Training accuracy as a function of iteration')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('training accuracy')\n",
    "    plt.show()\n",
    "    plt.plot(ind,v,'-go')\n",
    "    plt.title('Validation accuracy as a function of iteration')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('validation accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_hyperparameters(layer_1, layer_2, batch, rate, step):\n",
    "    best_avg = 0\n",
    "    l_1, l_2, bat, rat, stp = 0, 0, 0, 0, 0\n",
    "    for i in layer_1:\n",
    "        for j in layer_2:\n",
    "            for k in batch:\n",
    "                for l in rate:\n",
    "                    for m in step:\n",
    "                        train_accs, val_accs = [], []\n",
    "                        model = ThreeLayerNN(i, j)\n",
    "                        model.cuda()\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=l)\n",
    "                        for n in range(m):\n",
    "                            train(model, optimizer, k)\n",
    "                            if n % 100 == 0:\n",
    "                                train_accs.append(approx_train_accuracy(model))\n",
    "                                val_accs.append(val_accuracy(model))\n",
    "                        avg = np.mean(val_accs)\n",
    "                        print(avg ,\" \", k)\n",
    "                        if avg > best_avg:\n",
    "                            best_avg = avg\n",
    "                            l_1, l_2, bat, rat, stp = i, j, k, l, m\n",
    "                        #for s in model.children():\n",
    "                          #  s.reset_parameters()\n",
    "    \n",
    "    return l_1, l_2, bat, rat, stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_1 = [700]\n",
    "layer_2 = [50]\n",
    "batch = [32]\n",
    "rate = [0.00001]\n",
    "step = [10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-78ae70840b15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-220-22dabf908f57>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[1;34m(layer_1, layer_2, batch, rate, step)\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                 \u001b[0mtrain_accs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapprox_train_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-215-cac0f75d7f5e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JZ\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuned = tune_hyperparameters(layer_1, layer_2, batch, rate, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerNN(tuned[0], tuned[1])\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=tuned[3])                  \n",
    "train_accs, val_accs = [], []\n",
    "for i in range(tuned[4]):\n",
    "    train(model, optimizer, tuned[2])\n",
    "    if i % 100 == 0:\n",
    "        train_accs.append(approx_train_accuracy(model))\n",
    "        val_accs.append(val_accuracy(model))\n",
    "        print(\"%6d %5.2f %5.2f\" % (i, train_accs[-1], val_accs[-1]))\n",
    "print(np.mean(val_accs))\n",
    "plot_accuracies(train_accs, val_accs)\n",
    "print(tuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
